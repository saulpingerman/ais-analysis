{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c0f9c-09ae-44d0-83a7-1c0162bf6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pathlib, random, pandas as pd, geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "\n",
    "# ── 1. PICK A FINISHED PARQUET DAY ───────────────────────────\n",
    "PARQUET_DIR = pathlib.Path(\"~/ais_dk/parquet/2025\").expanduser()\n",
    "day_file = random.choice(list(PARQUET_DIR.rglob(\"2025-02-*.parquet\")))\n",
    "print(f\"Plotting {day_file}\")\n",
    "\n",
    "# ── 2. LOAD + LIGHT DOWNSAMPLE (120 k points ≈ fast & detailed) ────────────\n",
    "df = pd.read_parquet(day_file).sample(120_000, random_state=0)\n",
    "\n",
    "# ── 3. BUILD A GEO DATAFRAME & REPROJECT TO EPSG:3857 ──────────────────────\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
    "    crs=\"EPSG:4326\",   # WGS-84\n",
    ").to_crs(epsg=3857)    # Web-Mercator for contextily tiles\n",
    "\n",
    "# ── 4. PLOTTING ────────────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(9, 11))\n",
    "\n",
    "# scatter: color by SOG, small dots, slight transparency\n",
    "gdf.plot(\n",
    "    ax=ax,\n",
    "    column=\"sog\",\n",
    "    markersize=1.2,\n",
    "    alpha=0.6,\n",
    "    cmap=\"turbo\",          # vibrant perceptual colormap\n",
    "    legend=True,\n",
    "    legend_kwds={\"label\": \"Speed over ground (knots)\", \"shrink\": 0.6},\n",
    ")\n",
    "\n",
    "# add OpenStreetMap tiles (zoom-adaptive)\n",
    "cx.add_basemap(ax, crs=gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "ax.set_axis_off()\n",
    "ax.set_title(f\"AIS – {day_file.stem}\", fontsize=14, weight=\"bold\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# make sure the figures/ folder exists\n",
    "out_path = pathlib.Path(\"~/ais_dk/figures\").expanduser()\n",
    "out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "png = out_path / f\"ais_{day_file.stem}.png\"\n",
    "fig.savefig(png, dpi=300)\n",
    "print(\"✅  Saved\", png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e0771-e1d8-4adb-8089-e80ed62d9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Crisp OSM basemap + ONLY the 10 busiest MMSI tracks\n",
    "--------------------------------------------------\n",
    "• Reads one Parquet “day” file\n",
    "• Keeps the 10 MMSIs with the most rows\n",
    "• Plots each track as coloured dots (Tab10 palette)\n",
    "• Uses zoom-9 OSM tiles → sharp coastline detail\n",
    "• Crops to Denmark (lon 7-16 °E, lat 53-58 °N)\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "import pyproj\n",
    "\n",
    "# ── 1.  Parquet file you want to visualise ────────────────────────────────\n",
    "DAY_FILE = pathlib.Path(\n",
    "    \"~/ais_dk/parquet/2025/2025-02-01/2025-02-01.parquet\"\n",
    ").expanduser()\n",
    "\n",
    "# ── 2.  pull the 10 MMSIs with the most messages ───────────────────────────\n",
    "df = pd.read_parquet(DAY_FILE)\n",
    "top10 = df.mmsi.value_counts().head(10).index\n",
    "df = df[df.mmsi.isin(top10)]                           # keep only those rows\n",
    "\n",
    "# ── 3.  GeoDataFrame: WGS-84  →  Web-Mercator (EPSG:3857) ──────────────────\n",
    "gdf_wgs = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf = gdf_wgs.to_crs(epsg=3857)\n",
    "\n",
    "# ── 4.  bounding box for Denmark (EPSG:3857, in metres) ────────────────────\n",
    "proj = pyproj.Transformer.from_crs(4326, 3857, always_xy=True).transform\n",
    "x_min, y_min = proj(7.0, 53.0)     # SW corner\n",
    "x_max, y_max = proj(16.0, 58.0)    # NE corner\n",
    "\n",
    "# ── 5.  plot ───────────────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(10, 12), dpi=150)      # dpi↑ for print clarity\n",
    "\n",
    "palette = plt.cm.get_cmap(\"tab10\", len(top10))\n",
    "for i, mmsi in enumerate(top10):\n",
    "    tr = gdf[gdf.mmsi == mmsi]\n",
    "    ax.scatter(\n",
    "        tr.geometry.x,\n",
    "        tr.geometry.y,\n",
    "        s=3,                      # small points → coherent track\n",
    "        color=palette(i),\n",
    "        alpha=0.8,\n",
    "        label=str(mmsi),\n",
    "        linewidths=0,\n",
    "    )\n",
    "\n",
    "# High-resolution OSM tiles (zoom 9 feels right for DK)\n",
    "cx.add_basemap(\n",
    "    ax,\n",
    "    crs=gdf.crs,\n",
    "    source=cx.providers.OpenStreetMap.Mapnik,\n",
    "    zoom=9,\n",
    "    attribution_size=6,\n",
    ")\n",
    "\n",
    "# crop & annotate\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.set_title(f\"AIS — top-10 MMSI tracks — {DAY_FILE.stem}\", fontsize=14, weight=\"bold\")\n",
    "ax.legend(title=\"MMSI\", fontsize=8, frameon=True, loc=\"upper left\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# ── 6.  save ───────────────────────────────────────────────────────────────\n",
    "out_dir = pathlib.Path(\"~/ais_dk/figures\").expanduser()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_png = out_dir / f\"ais_top10_only_{DAY_FILE.stem}.png\"\n",
    "fig.savefig(out_png, dpi=300)\n",
    "print(\"✅  saved:\", out_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916edc57-8856-46d3-892e-da3da583d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "visualize_clean_vs_raw.py\n",
    "\n",
    "Load one Parquet “day” file of DMA AIS, apply the cleaning filters,\n",
    "and produce a side-by-side before/after scatter of lat/lon over Denmark.\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── Config ──────────────────────────────────────────────────────────────\n",
    "DAY = \"2025-02-01\"  # change to the date you want\n",
    "RAW_FILE = pathlib.Path(f\"~/ais_dk/parquet/2025/{DAY}/{DAY}.parquet\").expanduser()\n",
    "FIG_DIR  = pathlib.Path(\"~/ais_dk/figures\").expanduser()\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PNG  = FIG_DIR / f\"before_after_cleaning_{DAY}.png\"\n",
    "\n",
    "# ── 1. Load the raw data with Polars ─────────────────────────────────────\n",
    "df = pl.read_parquet(RAW_FILE).to_pandas()\n",
    "\n",
    "# ── 2. Apply cleaning filters ────────────────────────────────────────────\n",
    "df_clean = df[\n",
    "    (df.latitude.between(-90, 90)) &\n",
    "    (df.longitude.between(-180, 180)) &\n",
    "    ((df.latitude.abs() + df.longitude.abs()) > 0.1) &\n",
    "    (df.sog.between(0, 65)) &\n",
    "    (df.cog.between(0, 360)) &\n",
    "    (df.heading.between(0, 360))\n",
    "]\n",
    "\n",
    "# ── 3. Prepare the plot ─────────────────────────────────────────────────\n",
    "# Denmark bounding box in lon/lat\n",
    "MIN_LON, MAX_LON = 7.0, 16.0\n",
    "MIN_LAT, MAX_LAT = 53.0, 58.0\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), dpi=150)\n",
    "\n",
    "# Raw data\n",
    "ax1.scatter(df.longitude, df.latitude, s=1, color=\"gray\", alpha=0.5)\n",
    "ax1.set_title(f\"Raw AIS Data ({DAY})\")\n",
    "ax1.set_xlim(MIN_LON, MAX_LON)\n",
    "ax1.set_ylim(MIN_LAT, MAX_LAT)\n",
    "ax1.set_xlabel(\"Longitude\")\n",
    "ax1.set_ylabel(\"Latitude\")\n",
    "\n",
    "# Cleaned data\n",
    "ax2.scatter(df_clean.longitude, df_clean.latitude, s=1, color=\"tab:blue\", alpha=0.5)\n",
    "ax2.set_title(f\"Cleaned AIS Data ({DAY})\")\n",
    "ax2.set_xlim(MIN_LON, MAX_LON)\n",
    "ax2.set_ylim(MIN_LAT, MAX_LAT)\n",
    "ax2.set_xlabel(\"Longitude\")\n",
    "ax2.set_ylabel(\"Latitude\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# ── 4. Save the figure ─────────────────────────────────────────────────\n",
    "fig.savefig(OUT_PNG)\n",
    "print(\"Saved before/after cleaning visualization to:\", OUT_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923e12a-734b-4baa-8dd0-0b7b4131fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconnect or reuse your existing DuckDB connection\n",
    "import duckdb\n",
    "con = duckdb.connect(\"ais.duckdb\")\n",
    "\n",
    "# recreate ais_raw with the correct names and added year/month/day\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE ais_raw AS\n",
    "SELECT\n",
    "  mmsi,\n",
    "  \"timestamp\",\n",
    "  latitude  AS lat,\n",
    "  longitude AS lon,\n",
    "  sog,\n",
    "  cog,\n",
    "  heading,\n",
    "  ship_type,\n",
    "  EXTRACT(year  FROM \"timestamp\") AS year,\n",
    "  EXTRACT(month FROM \"timestamp\") AS month,\n",
    "  EXTRACT(day   FROM \"timestamp\") AS day\n",
    "FROM read_parquet('ais_dk/parquet/2025/*/*.parquet');\n",
    "\"\"\")\n",
    "\n",
    "# verify it worked\n",
    "print(con.execute(\"DESCRIBE ais_raw\").fetchall())\n",
    "print(con.execute(\"SELECT COUNT(*) FROM ais_raw\").fetchone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5bdb3-c2b1-484f-9fc4-97a2f99dfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "out_dir = pathlib.Path(\"partitioned_ais\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY ais_raw \n",
    "TO '{out_dir}/' \n",
    "(FORMAT PARQUET, PARTITION_BY (year, month, day));\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ca4d5-b4c0-46db-aeac-59ee08d799ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pathlib\n",
    "\n",
    "# 1) Connect\n",
    "con = duckdb.connect(\"data/ais.duckdb\")  # optional: keep the catalog inside `data/`\n",
    "\n",
    "# 2) Load & derive partitions from your moved raw files\n",
    "raw_pattern = \"data/ais_dk/parquet/2025/*/*.parquet\"\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE ais_raw AS\n",
    "SELECT\n",
    "  mmsi,\n",
    "  \"timestamp\",\n",
    "  latitude AS lat,\n",
    "  longitude AS lon,\n",
    "  sog,\n",
    "  cog,\n",
    "  heading,\n",
    "  ship_type,\n",
    "  EXTRACT(year  FROM \"timestamp\") AS year,\n",
    "  EXTRACT(month FROM \"timestamp\") AS month,\n",
    "  EXTRACT(day   FROM \"timestamp\") AS day\n",
    "FROM read_parquet('{raw_pattern}');\n",
    "\"\"\")\n",
    "\n",
    "# 3) Write out Hive-style, year/month/day partitions under data/partitioned_ais\n",
    "out_dir = pathlib.Path(\"data/partitioned_ais\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY ais_raw\n",
    "TO '{out_dir}/'\n",
    "(FORMAT PARQUET, PARTITION_BY (year, month, day));\n",
    "\"\"\")\n",
    "\n",
    "# 4) Verify\n",
    "print(\"Partitions written to:\", out_dir)\n",
    "for p in sorted(out_dir.rglob(\"*.parquet\"))[:5]:\n",
    "    print(\" \", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f424afe-12ba-4381-8c58-b9e81f418220",
   "metadata": {},
   "source": [
    "# This is the cleaner v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc5c62-b833-4c1b-9480-49c05f35b177",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import duckdb, pathlib\n",
    "\n",
    "# 1) Connect to (or create) your DuckDB catalog\n",
    "con = duckdb.connect(\"data/ais.duckdb\")\n",
    "\n",
    "# 2) Build a cleaned table\n",
    "con.execute(r\"\"\"\n",
    "CREATE OR REPLACE TABLE ais_clean AS\n",
    "WITH raw AS (\n",
    "  SELECT * \n",
    "  FROM read_parquet('data/partitioned_ais/*/*/*/*.parquet')\n",
    "),\n",
    "dedup AS (\n",
    "  -- remove exact duplicates of mmsi/timestamp/lat/lon\n",
    "  SELECT DISTINCT\n",
    "    mmsi, timestamp, lat, lon, sog, cog, heading, ship_type,\n",
    "    year, month, day\n",
    "  FROM raw\n",
    "),\n",
    "valid_geo AS (\n",
    "  -- drop nulls & out‐of‐bounds coords\n",
    "  SELECT *\n",
    "  FROM dedup\n",
    "  WHERE lat  BETWEEN -90  AND  90\n",
    "    AND lon  BETWEEN -180 AND 180\n",
    "    AND lat IS NOT NULL\n",
    "    AND lon IS NOT NULL\n",
    "),\n",
    "with_lag AS (\n",
    "  -- grab the previous point per vessel\n",
    "  SELECT\n",
    "    *,\n",
    "    LAG(lat)       OVER (PARTITION BY mmsi ORDER BY timestamp) AS lat_prev,\n",
    "    LAG(lon)       OVER (PARTITION BY mmsi ORDER BY timestamp) AS lon_prev,\n",
    "    LAG(timestamp) OVER (PARTITION BY mmsi ORDER BY timestamp) AS ts_prev\n",
    "  FROM valid_geo\n",
    "),\n",
    "with_speed AS (\n",
    "  -- compute Haversine‐distance / time to get knots\n",
    "  SELECT\n",
    "    *,\n",
    "    CASE\n",
    "      WHEN ts_prev IS NULL THEN NULL\n",
    "      ELSE\n",
    "        (\n",
    "          2 * 3440.065\n",
    "          * ASIN(\n",
    "              SQRT(\n",
    "                POWER(SIN((RADIANS(lat)      - RADIANS(lat_prev)) / 2), 2)\n",
    "                + COS(RADIANS(lat_prev)) * COS(RADIANS(lat))\n",
    "                  * POWER(SIN((RADIANS(lon) - RADIANS(lon_prev)) / 2), 2)\n",
    "              )\n",
    "            )\n",
    "        )\n",
    "        / (\n",
    "            (EXTRACT(EPOCH FROM timestamp)\n",
    "             - EXTRACT(EPOCH FROM ts_prev))\n",
    "            / 3600.0\n",
    "          )\n",
    "    END AS speed_kt\n",
    "  FROM with_lag\n",
    ")\n",
    "-- finally, keep only rows where speed ≤ 80 knots (or first‐point NULL speed)\n",
    "SELECT\n",
    "  mmsi, timestamp, lat, lon, sog, cog, heading, ship_type,\n",
    "  year, month, day\n",
    "FROM with_speed\n",
    "WHERE speed_kt IS NULL OR speed_kt <= 80;\n",
    "\"\"\")\n",
    "\n",
    "# 3) Write cleaned, year/month/day–partitioned Parquet out\n",
    "out_dir = pathlib.Path(\"data/cleaned_partitioned_ais\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY ais_clean\n",
    "TO '{out_dir}/'\n",
    "(FORMAT PARQUET, PARTITION_BY (year, month, day));\n",
    "\"\"\")\n",
    "\n",
    "# 4) Quick sanity‐check: list a few output files\n",
    "print(\"Written partitions:\")\n",
    "for p in sorted(out_dir.rglob(\"year=*/month=*/day=*/part-*.parquet\"))[:6]:\n",
    "    print(\" \", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f0d81-4b58-4ec7-b248-9ad71d2d9a53",
   "metadata": {},
   "source": [
    "# This is the cleaner v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d662e1a-57c2-4033-836b-72e64b462bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pathlib\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "\n",
    "# 1) Define haversine distance in nautical miles\n",
    "R_NM = 3440.065\n",
    "\n",
    "def haversine_nm(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Compute great-circle distance between (lat1, lon1) and (lat2, lon2) in nautical miles.\n",
    "    All inputs in decimal degrees.\n",
    "    \"\"\"\n",
    "    φ1, φ2 = radians(lat1), radians(lat2)\n",
    "    Δφ = radians(lat2 - lat1)\n",
    "    Δλ = radians(lon2 - lon1)\n",
    "    a = sin(Δφ/2)**2 + cos(φ1)*cos(φ2)*sin(Δλ/2)**2\n",
    "    return 2 * R_NM * asin(sqrt(a))\n",
    "\n",
    "# 2) Stateful filter: always compare to last accepted point\n",
    "def filter_track(df: pl.DataFrame, speed_thresh=80.0) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a Polars DataFrame for a single vessel (sorted by timestamp),\n",
    "    returns a new DataFrame with outliers (speed > threshold) removed,\n",
    "    comparing each fix against the last kept fix, preserving the original schema.\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    kept = {col: [] for col in cols}\n",
    "    last_lat = last_lon = None\n",
    "    last_ts = None\n",
    "\n",
    "    for row in df.rows():\n",
    "        ts = row[cols.index(\"timestamp\")]\n",
    "        lat = row[cols.index(\"lat\")]\n",
    "        lon = row[cols.index(\"lon\")]\n",
    "\n",
    "        # decide if we accept this point\n",
    "        if last_ts is None:\n",
    "            accept = True\n",
    "        else:\n",
    "            dt_h = (ts - last_ts).total_seconds() / 3600.0\n",
    "            if dt_h > 0:\n",
    "                speed = haversine_nm(last_lat, last_lon, lat, lon) / dt_h\n",
    "                accept = (speed <= speed_thresh)\n",
    "            else:\n",
    "                accept = False\n",
    "\n",
    "        if accept:\n",
    "            # record every column value\n",
    "            for i, col in enumerate(cols):\n",
    "                kept[col].append(row[i])\n",
    "            last_ts, last_lat, last_lon = ts, lat, lon\n",
    "\n",
    "    # build new DataFrame enforcing original schema\n",
    "    if kept[cols[0]]:\n",
    "        return pl.DataFrame(kept, schema=df.schema)\n",
    "    else:\n",
    "        # empty but same schema\n",
    "        return pl.DataFrame({col: [] for col in cols}, schema=df.schema)\n",
    "\n",
    "# 3) Iterate over daily partitions and apply cleaning\n",
    "input_root = pathlib.Path(\"data/partitioned_ais\")\n",
    "output_root = pathlib.Path(\"data/cleaned_partitioned_ais\")\n",
    "\n",
    "for year_dir in sorted(input_root.glob(\"year=*\")):\n",
    "    for month_dir in sorted(year_dir.glob(\"month=*\")):\n",
    "        for day_dir in sorted(month_dir.glob(\"day=*\")):\n",
    "            files = list(day_dir.glob(\"*.parquet\"))\n",
    "            if not files:\n",
    "                continue\n",
    "            df = pl.read_parquet([str(f) for f in files])\n",
    "\n",
    "            # a) dedupe\n",
    "            df = df.unique(subset=[\"mmsi\", \"timestamp\", \"lat\", \"lon\"])\n",
    "            # b) drop nulls & out-of-bounds coords\n",
    "            df = df.filter(\n",
    "                pl.col(\"lat\").is_not_null() & pl.col(\"lon\").is_not_null() &\n",
    "                pl.col(\"lat\").is_between(-90, 90) & pl.col(\"lon\").is_between(-180, 180)\n",
    "            )\n",
    "            # c) sort\n",
    "            df = df.sort([\"mmsi\", \"timestamp\"])\n",
    "\n",
    "            # d) group per vessel manually and filter each group\n",
    "            cleaned_parts = []\n",
    "            for m in df.get_column(\"mmsi\").unique().to_list():\n",
    "                grp = df.filter(pl.col(\"mmsi\") == m).sort(\"timestamp\")\n",
    "                cleaned_grp = filter_track(grp, speed_thresh=80.0)\n",
    "                if cleaned_grp.height > 0:\n",
    "                    cleaned_parts.append(cleaned_grp)\n",
    "\n",
    "            # e) concat or empty\n",
    "            if cleaned_parts:\n",
    "                cleaned = pl.concat(cleaned_parts, how=\"vertical\")\n",
    "            else:\n",
    "                cleaned = pl.DataFrame({col: [] for col in df.columns}, schema=df.schema)\n",
    "\n",
    "            # f) write back to cleaned partition structure\n",
    "            out_dir = output_root / year_dir.name / month_dir.name / day_dir.name\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            out_path = out_dir / \"part-0.parquet\"\n",
    "            cleaned.write_parquet(str(out_path))\n",
    "            print(f\"Cleaned {year_dir.name}/{month_dir.name}/{day_dir.name} -> {out_path}, rows: {cleaned.height}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3786012-be2e-4079-a491-cea7aee38dbb",
   "metadata": {},
   "source": [
    "# This is the cleaner v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0e969-e0a5-450b-b3cb-2c3c3ad482c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pathlib\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "SPEED_THRESH = 80.0    # max speed in knots\n",
    "GAP_HOURS   = 6        # split track when gap > this (hours)\n",
    "\n",
    "# Earth radius in nautical miles\n",
    "R_NM = 3440.065\n",
    "\n",
    "def haversine_nm(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"\n",
    "    Great-circle distance between two points (degrees) in nautical miles.\n",
    "    \"\"\"\n",
    "    φ1, φ2 = radians(lat1), radians(lat2)\n",
    "    Δφ = radians(lat2 - lat1)\n",
    "    Δλ = radians(lon2 - lon1)\n",
    "    a = sin(Δφ/2)**2 + cos(φ1)*cos(φ2)*sin(Δλ/2)**2\n",
    "    return 2 * R_NM * asin(sqrt(a))\n",
    "\n",
    "# Stateful speed filter\n",
    "def filter_track(df: pl.DataFrame, speed_thresh: float) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a Polars DataFrame for one vessel (sorted by 'timestamp'),\n",
    "    removes points where speed > threshold compared to last kept point.\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    kept = {c: [] for c in cols}\n",
    "    last_lat = last_lon = None\n",
    "    last_ts = None\n",
    "\n",
    "    for row in df.rows():\n",
    "        ts = row[cols.index('timestamp')]\n",
    "        lat = row[cols.index('lat')]\n",
    "        lon = row[cols.index('lon')]\n",
    "\n",
    "        if last_ts is None:\n",
    "            accept = True\n",
    "        else:\n",
    "            dt_h = (ts - last_ts).total_seconds() / 3600.0\n",
    "            accept = False\n",
    "            if dt_h > 0:\n",
    "                speed = haversine_nm(last_lat, last_lon, lat, lon) / dt_h\n",
    "                accept = (speed <= speed_thresh)\n",
    "\n",
    "        if accept:\n",
    "            for i, c in enumerate(cols):\n",
    "                kept[c].append(row[i])\n",
    "            last_ts, last_lat, last_lon = ts, lat, lon\n",
    "\n",
    "    if kept[cols[0]]:\n",
    "        return pl.DataFrame(kept, schema=df.schema)\n",
    "    else:\n",
    "        return pl.DataFrame({c: [] for c in cols}, schema=df.schema)\n",
    "\n",
    "# --- MAIN CLEANING LOOP WITH TRACK SPLITTING ACROSS DAYS ---\n",
    "state = {}  # mmsi -> (last_ts: datetime, last_track_id: int)\n",
    "\n",
    "input_root = pathlib.Path('data/partitioned_ais')\n",
    "output_root = pathlib.Path('data/cleaned_partitioned_ais')\n",
    "\n",
    "for year_dir in sorted(input_root.glob('year=*')):\n",
    "    for month_dir in sorted(year_dir.glob('month=*')):\n",
    "        for day_dir in sorted(month_dir.glob('day=*')):\n",
    "            files = list(day_dir.glob('*.parquet'))\n",
    "            if not files:\n",
    "                continue\n",
    "            df = pl.read_parquet([str(f) for f in files])\n",
    "\n",
    "            # a) dedupe & bounds\n",
    "            df = df.unique(subset=['mmsi','timestamp','lat','lon'])\n",
    "            df = df.filter(\n",
    "                pl.col('lat').is_not_null() & pl.col('lon').is_not_null() &\n",
    "                pl.col('lat').is_between(-90, 90) & pl.col('lon').is_between(-180, 180)\n",
    "            )\n",
    "            df = df.sort(['mmsi','timestamp'])\n",
    "\n",
    "            cleaned_parts = []\n",
    "            for m in df['mmsi'].unique().to_list():\n",
    "                grp = df.filter(pl.col('mmsi') == m).sort('timestamp')\n",
    "                if grp.is_empty():\n",
    "                    continue\n",
    "\n",
    "                # retrieve previous state\n",
    "                last_ts, offset = state.get(m, (None, 0))\n",
    "\n",
    "                # 1) speed filter\n",
    "                clean = filter_track(grp, SPEED_THRESH)\n",
    "                if clean.is_empty():\n",
    "                    continue\n",
    "\n",
    "                # 2) split into tracks with offset, carrying over across days\n",
    "                gap_flag = clean['timestamp'].diff() > pl.duration(hours=GAP_HOURS)\n",
    "                clean = clean.with_columns([\n",
    "                    gap_flag.cast(pl.Int32).alias('gap_int')\n",
    "                ])\n",
    "                clean = clean.with_columns([\n",
    "                    (pl.col('gap_int').cum_sum() + offset).alias('track_id')\n",
    "                ]).drop('gap_int')\n",
    "\n",
    "                                # 3) update state for next day\n",
    "                new_last_ts = clean['timestamp'].max()\n",
    "                # handle potential None if track_id is missing\n",
    "                raw_max = clean['track_id'].max()\n",
    "                new_offset = int(raw_max) if raw_max is not None else offset\n",
    "                state[m] = (new_last_ts, new_offset)\n",
    "\n",
    "                cleaned_parts.append(clean)\n",
    "\n",
    "            # concat all vessels for this day\n",
    "            if cleaned_parts:\n",
    "                cleaned = pl.concat(cleaned_parts, how='vertical')\n",
    "            else:\n",
    "                # empty DataFrame with track_id\n",
    "                cols = df.columns + ['track_id']\n",
    "                cleaned = pl.DataFrame({c: [] for c in cols})\n",
    "\n",
    "            # write out\n",
    "            out_dir = output_root / year_dir.name / month_dir.name / day_dir.name\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            cleaned.write_parquet(str(out_dir / 'part-0.parquet'))\n",
    "            print(f\"Processed {year_dir.name}/{month_dir.name}/{day_dir.name}: {cleaned.height} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df690d-eff0-4355-b1ec-c180c69e073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Option A: eager read of all files (OK if your dataset fits in memory)\n",
    "# df = pl.read_parquet(\"data/cleaned_partitioned_ais/*/*/*/*.parquet\")\n",
    "\n",
    "# Option B: lazy scan + collect (lets you push down filters, etc., before materializing)\n",
    "lf = pl.scan_parquet(\"data/cleaned_partitioned_ais/*/*/*/*.parquet\")\n",
    "df = lf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196dc163-45b7-4de6-b216-76d188c3eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mmsi'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636da6b9-d522-4ad2-907d-9099dac62275",
   "metadata": {},
   "source": [
    "# Random Track with ~20k posits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f5472-331b-4469-9b92-a1ca4b4a90d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# 1) filter and convert to Pandas\n",
    "pdf = (\n",
    "    df\n",
    "    .filter(pl.col(\"mmsi\") == 219031645)\n",
    "    .select([pl.col(\"lat\"), pl.col(\"lon\")])\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "# 2) center map on mean location\n",
    "center = [pdf['lat'].mean(), pdf['lon'].mean()]\n",
    "m = folium.Map(location=center, zoom_start=7)\n",
    "\n",
    "# 3) draw the track as a line\n",
    "coords = pdf[['lat','lon']].values.tolist()\n",
    "folium.PolyLine(coords, color='crimson', weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "# 4) add clustered points if you like\n",
    "mc = MarkerCluster().add_to(m)\n",
    "for _, row in pdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=3,\n",
    "        color='blue', fill=True\n",
    "    ).add_to(mc)\n",
    "\n",
    "# 5) display in Jupyter (or save to HTML)\n",
    "m  # in a notebook cell this will render the map\n",
    "# or: m.save(\"track_219031645.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039b202-c85e-4ea8-99d2-5e31333eaa40",
   "metadata": {},
   "source": [
    "# Track with 1st most posits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90f7ee-5a82-4a16-9c05-097a27622ef6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# 1) filter and convert to Pandas\n",
    "pdf = (\n",
    "    df\n",
    "    .filter(pl.col(\"mmsi\") == 2579999)\n",
    "    .select([pl.col(\"lat\"), pl.col(\"lon\")])\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "# 2) center map on mean location\n",
    "center = [pdf['lat'].mean(), pdf['lon'].mean()]\n",
    "m = folium.Map(location=center, zoom_start=7)\n",
    "\n",
    "# 3) draw the track as a line\n",
    "coords = pdf[['lat','lon']].values.tolist()\n",
    "folium.PolyLine(coords, color='crimson', weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "# 4) add clustered points if you like\n",
    "mc = MarkerCluster().add_to(m)\n",
    "for _, row in pdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=3,\n",
    "        color='blue', fill=True\n",
    "    ).add_to(mc)\n",
    "\n",
    "# 5) display in Jupyter (or save to HTML)\n",
    "m  # in a notebook cell this will render the map\n",
    "# or: m.save(\"track_219031645.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc629cbf-dcc3-439e-9608-b8d68196d39f",
   "metadata": {},
   "source": [
    "# Track with the 2nd most posits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acc86a-bf09-406a-a5b2-18c4275caf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# 1) filter and convert to Pandas\n",
    "pdf = (\n",
    "    df\n",
    "    .filter(pl.col(\"mmsi\") == 259027440)\n",
    "    .select([pl.col(\"lat\"), pl.col(\"lon\")])\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "# 2) center map on mean location\n",
    "center = [pdf['lat'].mean(), pdf['lon'].mean()]\n",
    "m = folium.Map(location=center, zoom_start=7)\n",
    "\n",
    "# 3) draw the track as a line\n",
    "coords = pdf[['lat','lon']].values.tolist()\n",
    "folium.PolyLine(coords, color='crimson', weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "# 4) add clustered points if you like\n",
    "mc = MarkerCluster().add_to(m)\n",
    "for _, row in pdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=3,\n",
    "        color='blue', fill=True\n",
    "    ).add_to(mc)\n",
    "\n",
    "# 5) display in Jupyter (or save to HTML)\n",
    "m  # in a notebook cell this will render the map\n",
    "# or: m.save(\"track_219031645.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d1cd3-7c6a-4ba3-bddf-b4bdb86dcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_track = df.filter(pl.col(\"mmsi\") == 219028133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b755f-0f45-47ce-930b-ad9a103ee81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5be5c-a267-4de6-a5bf-112d35569b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def add_velocity_knots(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a Polars DataFrame with columns ['mmsi','timestamp','lat','lon',…]\n",
    "    and returns a new DataFrame with an added 'velocity_knots' column,\n",
    "    computed as the great‐circle speed (in knots) between each point\n",
    "    and its predecessor for the same MMSI.\n",
    "    \"\"\"\n",
    "    R_NM = 3440.065  # Earth radius in nautical miles\n",
    "\n",
    "    return (\n",
    "        df\n",
    "        # 1) Ensure sorted by vessel and time\n",
    "        .sort([\"mmsi\", \"timestamp\"])\n",
    "        # 2) Grab the previous row’s lat/lon/timestamp per MMSI\n",
    "        .with_columns([\n",
    "            pl.col(\"lat\").shift(1).over(\"mmsi\").alias(\"lat_prev\"),\n",
    "            pl.col(\"lon\").shift(1).over(\"mmsi\").alias(\"lon_prev\"),\n",
    "            pl.col(\"timestamp\").shift(1).over(\"mmsi\").alias(\"ts_prev\"),\n",
    "        ])\n",
    "        # 3) Build the haversine “a” term\n",
    "        .with_columns([\n",
    "            (\n",
    "                ((pl.col(\"lat\").radians() - pl.col(\"lat_prev\").radians()) / 2).sin().pow(2)\n",
    "                + pl.col(\"lat_prev\").radians().cos()\n",
    "                  * pl.col(\"lat\").radians().cos()\n",
    "                  * ((pl.col(\"lon\").radians() - pl.col(\"lon_prev\").radians()) / 2)\n",
    "                    .sin()\n",
    "                    .pow(2)\n",
    "            ).alias(\"a\")\n",
    "        ])\n",
    "        # 4) Compute distance in nautical miles and delta time in hours\n",
    "        .with_columns([\n",
    "            # dist_nm = 2 * R_NM * arcsin(sqrt(a))\n",
    "            (2 * R_NM * pl.col(\"a\").sqrt().arcsin()).alias(\"dist_nm\"),\n",
    "            # dt_hrs = (timestamp - ts_prev) cast to Int64 ns, divided to hours\n",
    "            ((pl.col(\"timestamp\").cast(pl.Int64) - pl.col(\"ts_prev\").cast(pl.Int64))\n",
    "             / 3.6e12).alias(\"dt_hrs\"),\n",
    "        ])\n",
    "        # 5) Compute velocity in knots, null for first point or zero dt\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(\"dt_hrs\") > 0)\n",
    "              .then(pl.col(\"dist_nm\") / pl.col(\"dt_hrs\"))\n",
    "              .alias(\"velocity_knots\")\n",
    "        ])\n",
    "        # 6) Drop intermediates\n",
    "        .drop([\"lat_prev\", \"lon_prev\", \"ts_prev\", \"a\", \"dist_nm\", \"dt_hrs\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5811033-f721-4a2c-8baa-bee5b0de10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_track_with_vel = add_velocity_knots(single_track)\n",
    "\n",
    "single_track_with_vel.filter(pl.col(\"velocity_knots\") >= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28c176-b753-474e-8a08-298eb085582a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf0063-00fd-4d96-87af-66f0802fe1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
